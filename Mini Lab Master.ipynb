{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><h2><center>Mini-Lab: Logistic Regression and SVMs</center></h2></center></h2>\n",
    "<p><center>DS 7331</center></p>\n",
    "<p><center>Created by Sadik Aman, Dawn Bowerman, Zachary Harris, Alexandre Jasserme</center></p>\n",
    "\n",
    "<p><center>Sections of this code was adapted from: \n",
    "    <li>https://nbviewer.org/github/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#Libraries\n",
    "\n",
    "import plotly\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/owid-covid-data_modified.csv') # read in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   continent                       34965 non-null  object        \n",
      " 1   location                        34965 non-null  object        \n",
      " 2   date                            34965 non-null  datetime64[ns]\n",
      " 3   new_cases                       34965 non-null  float64       \n",
      " 4   new_cases_smoothed              34965 non-null  float64       \n",
      " 5   new_deaths                      34965 non-null  float64       \n",
      " 6   new_deaths_smoothed             34965 non-null  float64       \n",
      " 7   reproduction_rate               34965 non-null  float64       \n",
      " 8   new_vaccinations_smoothed       34965 non-null  float64       \n",
      " 9   new_people_vaccinated_smoothed  34965 non-null  float64       \n",
      " 10  stringency_index                34965 non-null  float64       \n",
      " 11  population                      34965 non-null  int64         \n",
      " 12  population_density              34965 non-null  float64       \n",
      " 13  median_age                      34965 non-null  float64       \n",
      " 14  aged_65_older                   34965 non-null  float64       \n",
      " 15  aged_70_older                   34965 non-null  float64       \n",
      " 16  gdp_per_capita                  34965 non-null  float64       \n",
      " 17  cardiovasc_death_rate           34965 non-null  float64       \n",
      " 18  diabetes_prevalence             34965 non-null  float64       \n",
      " 19  handwashing_facilities          34965 non-null  float64       \n",
      " 20  hospital_beds_per_thousand      34965 non-null  float64       \n",
      " 21  life_expectancy                 34965 non-null  float64       \n",
      " 22  human_development_index         34965 non-null  float64       \n",
      " 23  stringency_range                34944 non-null  object        \n",
      " 24  new_cases_range                 34965 non-null  object        \n",
      " 25  new_deaths_range                34965 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(19), int64(1), object(5)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Ideas from https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv\n",
    "\n",
    "# Sorting data frame by date column\n",
    "df['date'] = pd.to_datetime(df['date']) # Converting data columnn to datetime\n",
    "\n",
    "df = df.sort_values(by='date', ascending=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the table above, we eliminated particular variables because they would not be appropriate for the model. The dropped variables are as follows:\n",
    "\n",
    "*Unnamed: 0*: This variable was an index in the original dataset. This has no prediction value for the model.\n",
    "\n",
    "*new_deaths*: This variable was eliminated due to high correlation with new_deaths_range, our response variable.\n",
    "\n",
    "*new_deaths_smoothed*: This variable was eliminated for the same reason new_deaths was eliminated. \n",
    "\n",
    "*new_cases*: This variable was eliminated due to high correlation with new_deaths. \n",
    "\n",
    "*continent*: The variable type for this attribute is object.\n",
    "\n",
    "*location*: The variable type for this attribute is object.\n",
    "\n",
    "*stringency_range*:The variable type for this attribute is object.\n",
    "\n",
    "*new_cases_range*: This variable was created as a potential reponse variable. The variable type for this attribute is object.  \n",
    "\n",
    "*date*: While this variable was used to sort the dataset initially, it has to be eliminated when incorporating the dataset into the model due to the variable type.\n",
    "\n",
    "*new_people_vaccinated_smoothed*: This variable was eliminated due to high correlation with new_vaccinations_smoothed.\n",
    "\n",
    "*population_density*: This variable was eliminated due to high correlation with population.\n",
    "\n",
    "*aged_70_older*: This variable was eliminated due to high correlation with aged_65_older. \n",
    "\n",
    "*cardiovasc_death_rate*: This variable had a low logistic regression weight.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 14 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   new_cases_smoothed          34965 non-null  float64\n",
      " 1   reproduction_rate           34965 non-null  float64\n",
      " 2   new_vaccinations_smoothed   34965 non-null  float64\n",
      " 3   stringency_index            34965 non-null  float64\n",
      " 4   population                  34965 non-null  int64  \n",
      " 5   median_age                  34965 non-null  float64\n",
      " 6   aged_65_older               34965 non-null  float64\n",
      " 7   gdp_per_capita              34965 non-null  float64\n",
      " 8   diabetes_prevalence         34965 non-null  float64\n",
      " 9   handwashing_facilities      34965 non-null  float64\n",
      " 10  hospital_beds_per_thousand  34965 non-null  float64\n",
      " 11  life_expectancy             34965 non-null  float64\n",
      " 12  human_development_index     34965 non-null  float64\n",
      " 13  new_deaths_range            34965 non-null  object \n",
      "dtypes: float64(12), int64(1), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "float_df = df.drop([\"new_deaths\", \n",
    "                    \"new_deaths_smoothed\",\n",
    "                    \"new_cases\",\n",
    "                    \"continent\",\n",
    "                    \"location\",\n",
    "                    \"stringency_range\",\n",
    "                    \"new_cases_range\",\n",
    "                    \"date\",\n",
    "                    \"new_people_vaccinated_smoothed\",\n",
    "                    \"population_density\",\n",
    "                    \"aged_70_older\",\n",
    "                    \"cardiovasc_death_rate\"], axis=1)\n",
    "\n",
    "float_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to predict the X and y data as follows:\n",
    "if 'new_deaths_range' in float_df:\n",
    "    y = float_df['new_deaths_range'].values # get the labels\n",
    "    del float_df['new_deaths_range'] # get rid of the class label\n",
    "    X = float_df.values # use everything else to predict\n",
    "       ## X and y are now numpy matrices, by calling 'values' on the pandas data frames they \n",
    "    #    have converted into simple matrices to use with scikit learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reusable logisitic regression object\n",
    "#   setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, max_iter=500, solver='lbfgs', multi_class='multinomial' ) # get object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.5839148838805629\n",
      "confusion matrix\n",
      " [[2314  173  615]\n",
      " [ 364  381 1739]\n",
      " [ 265  481 2409]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6529001258437249\n",
      "confusion matrix\n",
      " [[2206  299  463]\n",
      " [ 205  256 1785]\n",
      " [ 187   95 3245]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6463791328223315\n",
      "confusion matrix\n",
      " [[2191  106  368]\n",
      " [ 397   73 1754]\n",
      " [ 414   52 3386]]\n"
     ]
    }
   ],
   "source": [
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "\n",
    "cv_object = TimeSeriesSplit( n_splits=3)\n",
    "\n",
    "# TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "for train_index, test_index in cv_object.split(X):\n",
    "   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "# accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat,labels=[\"high\",\"medium\",\"low\"] )\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_cases_smoothed has weight of 0.0007373063207444708\n",
      "reproduction_rate has weight of -2.658740724130506e-09\n",
      "new_vaccinations_smoothed has weight of 2.1578539238857453e-07\n",
      "stringency_index has weight of 1.0895031438689603e-06\n",
      "population has weight of -1.0040771442719033e-09\n",
      "median_age has weight of 4.802091558412188e-07\n",
      "aged_65_older has weight of 3.502564016756909e-07\n",
      "gdp_per_capita has weight of -3.3441886256070424e-05\n",
      "diabetes_prevalence has weight of -4.063690261350669e-08\n",
      "handwashing_facilities has weight of 1.4052003916775789e-06\n",
      "hospital_beds_per_thousand has weight of 6.069862620106982e-08\n",
      "life_expectancy has weight of 2.5563753238383275e-08\n",
      "human_development_index has weight of 6.113093659710626e-09\n"
     ]
    }
   ],
   "source": [
    "#interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = float_df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the logistic regression weights, we realized that new_cases_smoothed has a heavy influence on the model. This could create bias among the predictors. So, we dropped new_cases_smoothed from the dataset to compare both models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_df = float_df.drop([\"new_cases_smoothed\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to predict the X and y data as follows:\n",
    "if 'new_deaths_range' in float_df:\n",
    "    y = float_df['new_deaths_range'].values # get the labels\n",
    "    del float_df['new_deaths_range'] # get rid of the class label\n",
    "    X = float_df.values # use everything else to predict\n",
    "       ## X and y are now numpy matrices, by calling 'values' on the pandas data frames they \n",
    "    #    have converted into simple matrices to use with scikit learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.5839148838805629\n",
      "confusion matrix\n",
      " [[2314  173    0]\n",
      " [ 364  381    0]\n",
      " [   0    0    0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6529001258437249\n",
      "confusion matrix\n",
      " [[2206  299    0]\n",
      " [ 205  256    0]\n",
      " [   0    0    0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6463791328223315\n",
      "confusion matrix\n",
      " [[2191  106    0]\n",
      " [ 397   73    0]\n",
      " [   0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "\n",
    "cv_object = TimeSeriesSplit( n_splits=3)\n",
    "\n",
    "# TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "for train_index, test_index in cv_object.split(X):\n",
    "   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "# accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat, labels=[\"high\",\"medium\",\"\"])\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 12 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   reproduction_rate           34965 non-null  float64\n",
      " 1   new_vaccinations_smoothed   34965 non-null  float64\n",
      " 2   stringency_index            34965 non-null  float64\n",
      " 3   population                  34965 non-null  int64  \n",
      " 4   median_age                  34965 non-null  float64\n",
      " 5   aged_65_older               34965 non-null  float64\n",
      " 6   gdp_per_capita              34965 non-null  float64\n",
      " 7   diabetes_prevalence         34965 non-null  float64\n",
      " 8   handwashing_facilities      34965 non-null  float64\n",
      " 9   hospital_beds_per_thousand  34965 non-null  float64\n",
      " 10  life_expectancy             34965 non-null  float64\n",
      " 11  human_development_index     34965 non-null  float64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "float_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the new_cases_smoothed variable, we noticed that the model had increased difficulty classifying the medium death_range. We believe this is due to the model losing its strongest predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once normalized, te attributes weights should have magnitudes that reflect their predictive power in the logistic regression model.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6590779087060977\n",
      "[[2135  146  384]\n",
      " [ 221  449 1554]\n",
      " [ 215  460 3177]]\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, max_iter=500, solver='lbfgs', multi_class='multinomial' )\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat, labels = [\"high\",\"medium\",\"low\"])\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous two models, scaling the X_train data seems to improve the overall model. Results from this model show improved accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduction_rate has weight of 6.811952983286349\n",
      "new_vaccinations_smoothed has weight of -0.16465299913189227\n",
      "stringency_index has weight of 0.03985355901441289\n",
      "population has weight of 0.19786080264613945\n",
      "median_age has weight of 0.04494986691217253\n",
      "aged_65_older has weight of 0.20939868544000886\n",
      "gdp_per_capita has weight of 0.14792270710585903\n",
      "diabetes_prevalence has weight of -0.7476756652436289\n",
      "handwashing_facilities has weight of -0.029519022480499427\n",
      "hospital_beds_per_thousand has weight of 0.11663009949696555\n",
      "life_expectancy has weight of -0.09175642504238875\n",
      "human_development_index has weight of -0.08184057693604677\n"
     ]
    }
   ],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,float_df.columns) # combine attributes\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (13) does not match length of index (12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8a18809049b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;31m# create/copy the manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \"\"\"\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (13) does not match length of index (12)"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=float_df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.02, solver='liblinear') \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': float_df.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For more improvement and guarding against overfitting\n",
    "\n",
    "Xnew = float_df[['handwashing_facilities','hospital_beds_per_thousand']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline crossvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['handwashing_facilities','hospital_beds_per_thousand'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) \n",
    "# SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', decision_function_shape='ovo', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set predictions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "# print(svm_clf.coef_)\n",
    "# weights = pd.Series(svm_clf.coef_[0],index=float_df.columns)\n",
    "# weights.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the training data\n",
    "df_tested_on = float_df.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "df_support['new_deaths_range'] = y[svm_clf.support_] # add back in the 'new_deaths_range' Column to the pandas dataframe\n",
    "float_df['new_deaths_range'] = y # also add it back in for the original data\n",
    "df_support.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of the attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['new_deaths_range'])\n",
    "df_grouped = float_df.groupby(['new_deaths_range'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['reproduction_rate','new_vaccinations_smoothed','stringency_index','human_development_index']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(20,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['low','medium','high'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['low','medium','high'])\n",
    "    plt.title(v+' (Original)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
