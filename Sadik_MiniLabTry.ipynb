{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, SVMs, and Gradient Optimization\n",
    "\n",
    "*Please address questions to Professor Eric Larson, eclarson@smu.edu*\n",
    "\n",
    "In this notebook we will explore methods of using logistic regression in `scikit-learn` and we will also investigate methods for gradient descent. Finally we will look at using support vector machines and investigate parameters of kernel functions. A basic understanding of `scikit-learn` is required to complete this notebook, but we start very basic. Note also that there are more efficient methods of separating testing and training data, but we will leave that for a later lecture. \n",
    "\n",
    "First let's load a dataset and prepare it for analysis. We will use pandas to load in data, and then prepare it for classification. We will be using the titanic dataset (a very modest sized data set of about 1000 instances) before loading a larger, more complicated dataset for gradient descent methods.\n",
    "\n",
    "______\n",
    "The imputation methods used here are discussed in a previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/owid-covid-data_modified.csv') # read in the csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "Now let's look a little further at each of the categorical objects. Note that age range has already been saved as an ordinal. We need to look at `Sex` and `Embarked` objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `Sex`  attribute binary, there is no need to encode it using OneHotEncoding. We can just convert it to an integer. However, we should transform the `Embarked` attribute to take on three different values--one for each possible variable outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Training and Testing Split\n",
    "For training and testing purposes, let's gather the data we have and grab 80% of the instances for training and the remaining 20% for testing. Moreover, let's repeat this process of separating the testing and training data three times. We will use the hold out cross validation method built into `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>new_vaccinations_smoothed</th>\n",
       "      <th>...</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>stringency_range</th>\n",
       "      <th>new_cases_range</th>\n",
       "      <th>new_deaths_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>493</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>1557.0</td>\n",
       "      <td>1526.286</td>\n",
       "      <td>91.0</td>\n",
       "      <td>72.857</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>14115.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>37.746000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>40-50</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3515</th>\n",
       "      <td>16243</td>\n",
       "      <td>North America</td>\n",
       "      <td>Bermuda</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323548</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50669.315</td>\n",
       "      <td>139.547</td>\n",
       "      <td>13.00</td>\n",
       "      <td>11.404742</td>\n",
       "      <td>0.506452</td>\n",
       "      <td>82.59</td>\n",
       "      <td>0.545586</td>\n",
       "      <td>30-40</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13690</th>\n",
       "      <td>57963</td>\n",
       "      <td>North America</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>2855.0</td>\n",
       "      <td>1390.000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>55.429</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>8217.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7423.808</td>\n",
       "      <td>155.898</td>\n",
       "      <td>10.18</td>\n",
       "      <td>76.665000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>74.30</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>50-60</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21275</th>\n",
       "      <td>91453</td>\n",
       "      <td>North America</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>6081.0</td>\n",
       "      <td>4609.000</td>\n",
       "      <td>201.0</td>\n",
       "      <td>168.571</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>522893.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17336.469</td>\n",
       "      <td>152.783</td>\n",
       "      <td>13.06</td>\n",
       "      <td>87.847000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>75.05</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>40-50</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31265</th>\n",
       "      <td>136536</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>14170.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2896.913</td>\n",
       "      <td>427.698</td>\n",
       "      <td>7.11</td>\n",
       "      <td>72.704000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>71.10</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>30-40</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14429</th>\n",
       "      <td>60502</td>\n",
       "      <td>South America</td>\n",
       "      <td>Guyana</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>206.0</td>\n",
       "      <td>99.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.286</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>839.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7435.047</td>\n",
       "      <td>373.159</td>\n",
       "      <td>11.62</td>\n",
       "      <td>77.159000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>69.91</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>50-60</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>14433</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9604.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.000</td>\n",
       "      <td>1.340000</td>\n",
       "      <td>45720.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42658.576</td>\n",
       "      <td>114.898</td>\n",
       "      <td>4.29</td>\n",
       "      <td>90.078650</td>\n",
       "      <td>5.640000</td>\n",
       "      <td>81.63</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>40-50</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31819</th>\n",
       "      <td>138128</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>3011.0</td>\n",
       "      <td>2715.429</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.429</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>64643.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16277.671</td>\n",
       "      <td>109.861</td>\n",
       "      <td>7.04</td>\n",
       "      <td>90.670000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>77.15</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>40-50</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26269</th>\n",
       "      <td>113951</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>833.0</td>\n",
       "      <td>509.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2.230000</td>\n",
       "      <td>7892.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>116935.600</td>\n",
       "      <td>176.690</td>\n",
       "      <td>16.52</td>\n",
       "      <td>34.203248</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>80.23</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>30-40</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34964</th>\n",
       "      <td>154093</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>956.0</td>\n",
       "      <td>1495.429</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.857</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8788.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>36.791000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>50-60</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34965 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      continent     location       date  new_cases  \\\n",
       "0             493           Asia  Afghanistan 2021-07-01     1557.0   \n",
       "3515        16243  North America      Bermuda 2021-07-01        0.0   \n",
       "13690       57963  North America    Guatemala 2021-07-01     2855.0   \n",
       "21275       91453  North America       Mexico 2021-07-01     6081.0   \n",
       "31265      136536           Asia   Tajikistan 2021-07-01       24.0   \n",
       "...           ...            ...          ...        ...        ...   \n",
       "14429       60502  South America       Guyana 2022-01-01      206.0   \n",
       "3144        14433         Europe      Belgium 2022-01-01        0.0   \n",
       "31819      138128           Asia     Thailand 2022-01-01     3011.0   \n",
       "26269      113951           Asia        Qatar 2022-01-01      833.0   \n",
       "34964      154093         Africa     Zimbabwe 2022-01-01      956.0   \n",
       "\n",
       "       new_cases_smoothed  new_deaths  new_deaths_smoothed  reproduction_rate  \\\n",
       "0                1526.286        91.0               72.857           0.980000   \n",
       "3515                1.429         0.0                0.000           0.323548   \n",
       "13690            1390.000        67.0               55.429           1.160000   \n",
       "21275            4609.000       201.0              168.571           1.230000   \n",
       "31265              21.714         0.0                0.000           1.530000   \n",
       "...                   ...         ...                  ...                ...   \n",
       "14429              99.000         0.0                1.286           1.960000   \n",
       "3144             9604.571         0.0               26.000           1.340000   \n",
       "31819            2715.429        10.0               21.429           1.180000   \n",
       "26269             509.286         0.0                0.571           2.230000   \n",
       "34964            1495.429        13.0               18.857           0.800000   \n",
       "\n",
       "       new_vaccinations_smoothed  ...  gdp_per_capita  cardiovasc_death_rate  \\\n",
       "0                   14115.000000  ...        1803.987                597.029   \n",
       "3515                  430.000000  ...       50669.315                139.547   \n",
       "13690                8217.000000  ...        7423.808                155.898   \n",
       "21275              522893.000000  ...       17336.469                152.783   \n",
       "31265               14170.000000  ...        2896.913                427.698   \n",
       "...                          ...  ...             ...                    ...   \n",
       "14429                 839.000000  ...        7435.047                373.159   \n",
       "3144                45720.000000  ...       42658.576                114.898   \n",
       "31819               64643.666667  ...       16277.671                109.861   \n",
       "26269                7892.000000  ...      116935.600                176.690   \n",
       "34964                8788.000000  ...        1899.775                307.846   \n",
       "\n",
       "       diabetes_prevalence  handwashing_facilities  \\\n",
       "0                     9.59               37.746000   \n",
       "3515                 13.00               11.404742   \n",
       "13690                10.18               76.665000   \n",
       "21275                13.06               87.847000   \n",
       "31265                 7.11               72.704000   \n",
       "...                    ...                     ...   \n",
       "14429                11.62               77.159000   \n",
       "3144                  4.29               90.078650   \n",
       "31819                 7.04               90.670000   \n",
       "26269                16.52               34.203248   \n",
       "34964                 1.82               36.791000   \n",
       "\n",
       "       hospital_beds_per_thousand  life_expectancy  human_development_index  \\\n",
       "0                        0.500000            64.83                 0.511000   \n",
       "3515                     0.506452            82.59                 0.545586   \n",
       "13690                    0.600000            74.30                 0.663000   \n",
       "21275                    1.380000            75.05                 0.779000   \n",
       "31265                    4.800000            71.10                 0.668000   \n",
       "...                           ...              ...                      ...   \n",
       "14429                    1.600000            69.91                 0.682000   \n",
       "3144                     5.640000            81.63                 0.931000   \n",
       "31819                    2.100000            77.15                 0.777000   \n",
       "26269                    1.200000            80.23                 0.848000   \n",
       "34964                    1.700000            61.49                 0.571000   \n",
       "\n",
       "       stringency_range  new_cases_range  new_deaths_range  \n",
       "0                 40-50             high              high  \n",
       "3515              30-40              low               low  \n",
       "13690             50-60             high              high  \n",
       "21275             40-50             high              high  \n",
       "31265             30-40              low               low  \n",
       "...                 ...              ...               ...  \n",
       "14429             50-60           medium               low  \n",
       "3144              40-50              low               low  \n",
       "31819             40-50             high              high  \n",
       "26269             30-40             high               low  \n",
       "34964             50-60             high              high  \n",
       "\n",
       "[34965 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ideas from https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv\n",
    "\n",
    "# Sorting data frame by date column\n",
    "df['date'] = pd.to_datetime(df['date']) # Converting data columnn to datetime\n",
    "\n",
    "df = df.sort_values(by='date', ascending=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'continent', 'location', 'date', 'new_cases', 'new_cases_smoothed', 'new_deaths', 'new_deaths_smoothed', 'reproduction_rate', 'new_vaccinations_smoothed', 'new_people_vaccinated_smoothed', 'stringency_index', 'population', 'population_density', 'median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'cardiovasc_death_rate', 'diabetes_prevalence', 'handwashing_facilities', 'hospital_beds_per_thousand', 'life_expectancy', 'human_development_index', 'stringency_range', 'new_cases_range', 'new_deaths_range']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#number = LableEncoder()\n",
    "#train[''] = number.fit_transform(train[''].astype('str'))\n",
    "#test[''] = number.fit_transform(test[''].astype('str'))\n",
    "#train.head(5)\n",
    "\n",
    "#columns = “new_cases new_deaths new_cases_smoothed new_deaths_smoothed reproduction_rate cardiovasc_death_rate handwashing_facilities stringency_range \thuman_development_index”.split() # Declare the columns names\n",
    "#df = pd.DataFrame(df.data, columns=columns) # load the dataset as a pandas data frame\n",
    "#dataset = df()\n",
    "#df = df.drop(columns=['continent', 'location','stringency_range','new_cases_range','new_deaths_range','new_vaccinations_smoothed','new_people_vaccinated_smoothed','population','population_density','hospital_beds_per_thousand'])\n",
    "#df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "#df['date'] = pd.to_numeric(pd.to_datetime(df['date']))\n",
    "\n",
    "#df2=df.drop(['date','continent','location','new_deaths_range','stringency_range','new_cases_range'], axis=1)\n",
    "#df2\n",
    "\n",
    "df2 = df.drop([\"continent\",\"date\",\"location\",\"stringency_range\",\"new_cases_range\"], axis=1)\n",
    "df2\n",
    "\n",
    "#del df['date']\n",
    "#del df['location']\n",
    "#del df['new_deaths_range']\n",
    "#del df['stringency_range']\n",
    "#del df['new_cases_range']\n",
    "#del df['continent']\n",
    "\n",
    "#y = df.new_cases # define the target variable (dependent variable) as y\n",
    "#del df['new_cases'] # get rid of the class label\n",
    "#X = df.values # use everything else to predict!\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "#if 'new_deaths_range' in float_df:\n",
    "#    y = float_df['new_deaths_range'].values # get the labels we want\n",
    "#   del float_df['new_deaths_range'] # get rid of the class label\n",
    "#   X = float_df.values # use everything else to predict!\n",
    "#      ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "\n",
    "# create training and testing vars\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "#print(X_train.shape, y_train.shape)\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "# i have commented out below code\n",
    "# we want to predict the X and y data as follows:\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'new_deaths_range' in df2:\n",
    "    y = df2['new_deaths_range'].values # get the labels we want\n",
    "    del df2['new_deaths_range'] # get rid of the class label\n",
    "    X = df2.values # use everything else to predict!\n",
    "       ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "        \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                       test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Logistic Regression\n",
    "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.6437866437866437\n",
      "confusion matrix\n",
      " [[1815  471   37]\n",
      " [  99 2669   11]\n",
      " [ 233 1640   18]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6424996424996425\n",
      "confusion matrix\n",
      " [[1790  546    7]\n",
      " [  91 2699    1]\n",
      " [ 215 1640    4]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.642070642070642\n",
      "confusion matrix\n",
      " [[1791  525   25]\n",
      " [ 115 2690    1]\n",
      " [ 227 1610    9]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear') # get object\n",
    "#lr_clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "# Converting string to float\n",
    "#df['continent'] = pd.to_numeric(df['continent'])\n",
    "#df['continent'] = df['continent'].astype(int)\n",
    "#df['continent'] = pd.to_numeric(df['continent'])\n",
    "\n",
    "#df['continent'] = df['continent'].astype(float)\n",
    "#df['location'] = df['location'].astype(float)\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y):   \n",
    "# I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    " # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    " # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.6330616330616331\n",
      "confusion matrix\n",
      " [[1702  618    0]\n",
      " [ 107 2725    0]\n",
      " [ 172 1669    0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.636922636922637\n",
      "confusion matrix\n",
      " [[1715  545    0]\n",
      " [  98 2739    0]\n",
      " [ 220 1676    0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6593736593736593\n",
      "confusion matrix\n",
      " [[1845  423  106]\n",
      " [ 108 2680   13]\n",
      " [ 241 1491   86]]\n"
     ]
    }
   ],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65408265 0.64106964 0.65594166]\n"
     ]
    }
   ],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09f384626cc436eb3dcd3bd38cd2f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.451, description='cost', max=5.0, min=0.001, step=0.05), Output()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor(cost)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None,solver='liblinear') # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting weights\n",
    "Okay, so now lets take the last trained model for logistic regression and try to interpret the weights for the model. Is there something about the weights that makes this model more interpretable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 has weight of -1.078309402315385e-05\n",
      "continent has weight of 0.0005209026078943367\n",
      "location has weight of 0.0004188316689318398\n",
      "date has weight of 1.61547267766295e-05\n",
      "new_cases has weight of 1.2918000843183024e-05\n",
      "new_cases_smoothed has weight of -1.5436618941233674e-07\n",
      "new_deaths has weight of -2.660914366613069e-07\n",
      "new_deaths_smoothed has weight of 1.1842238335490325e-06\n",
      "reproduction_rate has weight of -2.7826255977150727e-06\n",
      "new_vaccinations_smoothed has weight of -2.0622984603898725e-09\n",
      "new_people_vaccinated_smoothed has weight of -7.785761439742199e-05\n",
      "stringency_index has weight of -1.4429996405887245e-06\n",
      "population has weight of 3.734695632118015e-07\n",
      "population_density has weight of 2.5263797941706303e-07\n",
      "median_age has weight of -4.44866271739712e-05\n",
      "aged_65_older has weight of -3.7092644541476205e-05\n",
      "aged_70_older has weight of -8.271329651446903e-07\n",
      "gdp_per_capita has weight of -6.444466718350313e-07\n",
      "cardiovasc_death_rate has weight of -4.352029767374501e-08\n",
      "diabetes_prevalence has weight of -8.153549437459327e-06\n",
      "handwashing_facilities has weight of -5.623143970487676e-08\n"
     ]
    }
   ],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weight interpretations **are not neccessarily interpretable** because of the values we had. Very large attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same dynamic range. Once we normalize the attributes, the weights should have magnitudes that reflect their poredictive power in the logistic regression model.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7225797225797226\n",
      "[[2003  247   91]\n",
      " [  46 2471  289]\n",
      " [  41 1226  579]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21480/2452895501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# sort these attributes and spit them out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mzip_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# combine attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mzip_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has weight of'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# now print them out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear') # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more improvement and guarding against overfitting:** At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear') \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': df_imputed.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more improvement and guarding against overfitting:** At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df_imputed[['Age','Pclass','IsMale']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['Age','Pclass','IsMale'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "We can use the previous training and testing attributes (scaled) to investigate the weights and support vectors in the attributes. SVMs were first hypothesized by Vladmir Vapnik ~50 years ago, but did not gain popularity until the turn of the millenium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Here he is, in all his glory:\n",
    "Image(url='http://engineering.columbia.edu/files/engineering/vapnik.jpg')\n",
    "# Image(url='http://yann.lecun.com/ex/images/allyourbayes.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = df_imputed.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "df_support['Survived'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "df_imputed['Survived'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['Survived'])\n",
    "df_grouped = df_imputed.groupby(['Survived'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['Age','Pclass','IsMale','FamilySize']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Perished','Survived'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Perished','Survived'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the analysis here is basically telling us what the original statistics of the data looked like, and also what the statistics of the support vectors looked like. We can see that the separation in distributions is not as great as the separation for the original data. This is because the support vectors tend to be instances on the edge of the class boundaries and also instances that are classified incorrectly in the training data. \n",
    "\n",
    "You can also look at joint plots of the data and see how relationships have changed. (**Hint hint for the min-lab assignment**--this would be a nice analysis of the support vectors.)\n",
    "\n",
    "That's mostly it for using these things! They are really nice analysis tools and provide human interpretable summaries of the data. \n",
    "___\n",
    "\n",
    "# Gradient Based Alternatives\n",
    "So now let's go and find out how we can use these when our data size gets bigger. Like a lot bigger. We will use a kaggle dataset that attempts to classify plankton. We will use some example code to get us started from the tutorial here:\n",
    "http://www.kaggle.com/c/datasciencebowl/details/tutorial \n",
    "\n",
    "You can download from Kaggle (login required): https://www.kaggle.com/c/datasciencebowl/data\n",
    "\n",
    "UPDATE: This problem was also solved using deep learning! Check out the blog here:\n",
    "http://benanne.github.io/2015/03/17/plankton.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/Plankton-Diagram3-lg.png')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load another dataset (large) and train using various methods of gradient (and mini-batch)\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# change this to point to the dataset on your machine/cluster!!\n",
    "# For my students: please email me for a link to the data if you cannot get it from Kaggle\n",
    "directory_of_dataset = \"D:/SMU/Larson/2019 Data Mining 7331 Updates/Class Github/kaggle_plank/\"\n",
    "\n",
    "# get the classnames from the training data directory structure\n",
    "directory_names = list(set(glob.glob(os.path.join(directory_of_dataset,\"train\", \"*\"))\n",
    " ).difference(set(glob.glob(os.path.join(directory_of_dataset,\"train\",\"*.*\")))))\n",
    "\n",
    "print('number of classes:', len(directory_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this code requires the use of skimage to process the images (you will need to install via pip)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Rescale the images and create the combined metrics and training labels\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "\n",
    "#get the total training images\n",
    "numberofImages = 0\n",
    "for folder in directory_names:\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "             # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            numberofImages += 1\n",
    "\n",
    "# We'll rescale the images to be 40x40\n",
    "maxPixel = 25\n",
    "imageSize = maxPixel * maxPixel\n",
    "num_rows = numberofImages # one row for each image in the training dataset\n",
    "num_features = imageSize # for our ratio\n",
    "\n",
    "# X is the feature vector with one row of features per image\n",
    "# consisting of the pixel values and our metric\n",
    "X = np.zeros((num_rows, num_features), dtype=float)\n",
    "# y is the numeric class label \n",
    "y = np.zeros((num_rows))\n",
    "\n",
    "files = []\n",
    "# Generate training data\n",
    "i = 0    \n",
    "label = 0\n",
    "# List of string of class names\n",
    "namesClasses = list()\n",
    "\n",
    "print(\"Reading images\")\n",
    "# Navigate through the list of directories\n",
    "for folder in directory_names:\n",
    "    # Append the string class name for each class\n",
    "    currentClass = folder.split(os.pathsep)[-1]\n",
    "    namesClasses.append(currentClass)\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "            # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            \n",
    "            # Read in the images and create the features\n",
    "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)            \n",
    "            image = imread(nameFileImage, as_gray=True)\n",
    "            files.append(nameFileImage)\n",
    "            #axisratio = getMinorMajorRatio(image)\n",
    "            image = resize(image, (maxPixel, maxPixel))\n",
    "            \n",
    "            # Store the rescaled image pixels and the axis ratio\n",
    "            X[i, 0:imageSize] = np.reshape(image, (1, imageSize))\n",
    "            #X[i, imageSize] = axisratio\n",
    "            \n",
    "            # Store the classlabel\n",
    "            y[i] = label\n",
    "            i += 1\n",
    "            # report progress for each 5% done  \n",
    "            report = [int((j+1)*num_rows/20.) for j in range(20)]\n",
    "            if i in report: print(np.ceil(i *100.0 / num_rows), \"% done\")\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where the online tutorial code stops and my code starts\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now divide the data into test and train using scikit learn built-ins\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "\n",
    "cv = StratifiedShuffleSplit( n_splits=1,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use some compact notation for creating a linear SVM classifier with stichastic descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "regularize_const = 0.1\n",
    "iterations = 5\n",
    "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "    svm_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = svm_sgd.predict(scl.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print('SVM:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use some compact notation for creating a logistic regression classifier with stochastic descent\n",
    "log_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "    log_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = log_sgd.predict(scl.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print('Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets use some of what we know from this class to reduce the dimensionality of the set\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 50\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "\n",
    "iterations = 150\n",
    "log_sgd = SGDClassifier(\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "# you could also set this up in a pipeline\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "    log_sgd.fit(pca.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = log_sgd.predict(pca.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print('Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "plt.imshow(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition does not use \"accuracy\" as the evaluation of the best model; they use the log loss:\n",
    "$$logloss=-\\frac{1}{N}\\sum_{i=1}^m\\sum_{j=1}^C {\\bf 1}_{ij}\\ln(p_{ij})$$\n",
    "\n",
    "Where there are $m$ instances (images) in the dataset, and $C$ is the number of classes. The equation ${\\bf 1}_{ij}$ is an indicator function that ensures we only add log probabilities when the class is correct. That is, it is zero if the predicted class for the $i^{th}$ instance is not equal to $j$ and it is one when the class of the $i^{th}$ instance == $j$. To prevent extremities in the log function they also replace probabilities, $p$, with $p=\\max(\\min(p,1-10^{-15}),10^{-15})$\n",
    "\n",
    "Would this be easy to code in python? `scikit-learn` has an implementation for log loss, but it is not exactly what the competition uses and is only defined for binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the tutorial: http://www.kaggle.com/c/datasciencebowl/details/tutorial \n",
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "            true class, integers in [0, n_classes - 1)\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    n_samples = actual.shape[0]\n",
    "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
    "    vectsum = np.sum(actual * np.log(predictions))\n",
    "    loss = -1.0 / n_samples * vectsum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## How do you think you might increase the accuracy of the classifier(s)?\n",
    "\n",
    "Normalize your data. Try to reduce the dimensions as much as you can.\n",
    "#https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/\n",
    "-Add more data\n",
    "-Treat missing and Outlier values\n",
    "-Feature Engineering : This step helps to extract more information from existing data. New information is extracted in terms of new features. These features may have a higher ability to explain the variance in the training data. Thus, giving improved model accuracy.Feature engineering is highly influenced by hypotheses generation. Good hypothesis result in good features. That’s why, I always suggest to invest quality time in hypothesis generation. Feature engineering process can be divided into two steps\n",
    "-Feature Selection : \n",
    "- Multiple algorithms\n",
    "-Hitting at the right machine learning algorithm is the ideal approach to achieve higher accuracy: Regression, classification and clustering\n",
    "- Algorithm Tuning : We know that machine learning algorithms are driven by parameters. These parameters majorly influence the outcome of learning process.The objective of parameter tuning is to find the optimum value for each parameter to improve the accuracy of the model. To tune these parameters, you must have a good understanding of these meaning and their individual impact on model. You can repeat this process with a number of well performing models.For example: In random forest, we have various parameters like max_features, number_trees, random_state, oob_score and others. Intuitive optimization of these parameter values will result in better and more accurate models.\n",
    "- Ensemble methods: This is the most common approach found majorly in winning solutions of Data science competitions. This technique simply combines the result of multiple weak models and produce better results. This can be achieved through many ways:Bagging (Bootstrap Aggregating), Boosting.\n",
    "- Cross Validation\n",
    "\n",
    "- Search through parameters for models?\n",
    "- Try different classifiers?\n",
    "- Add more features (through better image processing)?\n",
    "\n",
    "___\n",
    "## How do you think we can make the algorithms more efficient for training/testing?\n",
    "- What about mini-batch training? \n",
    "- Sampling?\n",
    "- Map/Reduce (what are advantages/disadvantages)?\n",
    "- Buy a ton of memory on AWS virtual machines?\n",
    "\n",
    "**Note:** For mini-batch calculations (they are not really needed here because the dataset fits in memory) they can be accessed for a number of different classifiers (including SGDClassifier) by managing the sub-samples we send it, $X_{sub}$, and calling the function `partial_fit`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "4c41474cfeee4f77959ec7a366881a57": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
